[hanlp]
# hanlp 的 jar 路径和文件路径
classpath = /niub/www/sourcecode/hanlp/hanlp.jar:/niub/www/sourcecode/hanlp/

[crf]
# CRF模型文件
model = /crf/crf_model
# CRF模板文件
template = /crf/crf_template
# Train数据
train_file = /crf/train_data
# Test数据
test_file = /crf/test_data

[sql_dict]
# 线上字典库 IP
host = 10.140.85.247
# 线上字典库 Port
port = 6633
# 线上字典库 数据库名字
db = search_word'
# 线上字典库 表名
table = dict_classification
# 线上字典库 用户
user = team_dict_ro
# 线上字典库 密码
password = 8f7c6535b895

[dictionary]
# 配置不同业务中的不可切分的词及其标签类型，注意，下面业务同时只能有一个生效，当一个生效时，请注释掉其他业务，否则会覆盖掉
# 「资讯搜索」业务中不可切分的词及其标签类型，比如 '新闻   useless'
# phrase = /dict/news_phrase
# 「图片搜索」业务中不可切分的词及其标签类型，比如 '走势   useless'
phrase = /dict/charts_phrase

[data]
# 统计数据，包括左右熵，TF，IDF
Entropy_data = /data/Entropy.data
TF_data= /data/TF.data
IDF_data = /data/IDF.data

[xgboost]
# xgboost 模型文件
model = /xgboost/xgboost_model
# 人工标注的数据文件
data = /xgboost/all_data
# 人工标注的数据文件中分出的训练数据
data_train = /xgboost/all_data_train
# 人工标注的数据文件中分出的测试数据
data_test = /xgboost/all_data_test
# xgboost训练数据
train = /xgboost/train
# xgboost测试数据
test = /xgboost/test

[word2vec]
model = /word2vec/w2v_title_50_3.model
sentences = /word2vec/sentences
